// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --version 5
// RUN: %clang_cc1 -O0 -triple arm64-apple-iphoneos -fbounds-safety -fbounds-safety-bringup-missing-checks=return_size -emit-llvm %s -o - | FileCheck %s
// RUN: %clang_cc1 -O0 -triple arm64-apple-iphoneos -fbounds-safety -fno-bounds-safety-bringup-missing-checks=return_size -emit-llvm %s -o - | FileCheck --check-prefix=LEGACY %s
#include "bounds-attributed-in-return-null-system-header.h"

// FIXME: The IR for `inline_header_func_unspecified_ptr` and
// `inline_header_func_unsafe_indexable_ptr` are not included by
// `utils/update_cc_test_checks.py` but that's the IR we **actually** want to
// match. There doesn't seem to be a way to persuade the script to generate
// CHECK lines for it.

void consume(int* __bidi_indexable);

// CHECK-LABEL: define dso_local void @use_inline_header_func_unspecified_ptr(
// CHECK-SAME: i32 noundef [[COUNT:%.*]]) #[[ATTR0:[0-9]+]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[COUNT_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[PTR:%.*]] = alloca %"__bounds_safety::wide_ptr.bidi_indexable", align 8
// CHECK-NEXT:    [[BYVAL_TEMP:%.*]] = alloca %"__bounds_safety::wide_ptr.bidi_indexable", align 8
// CHECK-NEXT:    store i32 [[COUNT]], ptr [[COUNT_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[COUNT_ADDR]], align 4
// CHECK-NEXT:    [[CALL:%.*]] = call ptr @inline_header_func_unspecified_ptr(i32 noundef [[TMP0]])
// CHECK-NEXT:    [[IDX_EXT:%.*]] = sext i32 [[TMP0]] to i64
// CHECK-NEXT:    [[ADD_PTR:%.*]] = getelementptr inbounds i32, ptr [[CALL]], i64 [[IDX_EXT]]
// CHECK-NEXT:    [[TMP1:%.*]] = getelementptr inbounds %"__bounds_safety::wide_ptr.bidi_indexable", ptr [[PTR]], i32 0, i32 0
// CHECK-NEXT:    store ptr [[CALL]], ptr [[TMP1]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds %"__bounds_safety::wide_ptr.bidi_indexable", ptr [[PTR]], i32 0, i32 1
// CHECK-NEXT:    store ptr [[ADD_PTR]], ptr [[TMP2]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds %"__bounds_safety::wide_ptr.bidi_indexable", ptr [[PTR]], i32 0, i32 2
// CHECK-NEXT:    store ptr [[CALL]], ptr [[TMP3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 8 [[BYVAL_TEMP]], ptr align 8 [[PTR]], i64 24, i1 false)
// CHECK-NEXT:    call void @consume(ptr noundef [[BYVAL_TEMP]])
// CHECK-NEXT:    ret void
//
// LEGACY-LABEL: define dso_local void @use_inline_header_func_unspecified_ptr(
// LEGACY-SAME: i32 noundef [[COUNT:%.*]]) #[[ATTR0:[0-9]+]] {
// LEGACY-NEXT:  [[ENTRY:.*:]]
// LEGACY-NEXT:    [[COUNT_ADDR:%.*]] = alloca i32, align 4
// LEGACY-NEXT:    [[PTR:%.*]] = alloca %"__bounds_safety::wide_ptr.bidi_indexable", align 8
// LEGACY-NEXT:    [[BYVAL_TEMP:%.*]] = alloca %"__bounds_safety::wide_ptr.bidi_indexable", align 8
// LEGACY-NEXT:    store i32 [[COUNT]], ptr [[COUNT_ADDR]], align 4
// LEGACY-NEXT:    [[TMP0:%.*]] = load i32, ptr [[COUNT_ADDR]], align 4
// LEGACY-NEXT:    [[CALL:%.*]] = call ptr @inline_header_func_unspecified_ptr(i32 noundef [[TMP0]])
// LEGACY-NEXT:    [[IDX_EXT:%.*]] = sext i32 [[TMP0]] to i64
// LEGACY-NEXT:    [[ADD_PTR:%.*]] = getelementptr inbounds i32, ptr [[CALL]], i64 [[IDX_EXT]]
// LEGACY-NEXT:    [[TMP1:%.*]] = getelementptr inbounds %"__bounds_safety::wide_ptr.bidi_indexable", ptr [[PTR]], i32 0, i32 0
// LEGACY-NEXT:    store ptr [[CALL]], ptr [[TMP1]], align 8
// LEGACY-NEXT:    [[TMP2:%.*]] = getelementptr inbounds %"__bounds_safety::wide_ptr.bidi_indexable", ptr [[PTR]], i32 0, i32 1
// LEGACY-NEXT:    store ptr [[ADD_PTR]], ptr [[TMP2]], align 8
// LEGACY-NEXT:    [[TMP3:%.*]] = getelementptr inbounds %"__bounds_safety::wide_ptr.bidi_indexable", ptr [[PTR]], i32 0, i32 2
// LEGACY-NEXT:    store ptr [[CALL]], ptr [[TMP3]], align 8
// LEGACY-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 8 [[BYVAL_TEMP]], ptr align 8 [[PTR]], i64 24, i1 false)
// LEGACY-NEXT:    call void @consume(ptr noundef [[BYVAL_TEMP]])
// LEGACY-NEXT:    ret void
//
void use_inline_header_func_unspecified_ptr(int count) {
  int* ptr = inline_header_func_unspecified_ptr(count);
  consume(ptr);
}

// CHECK-LABEL: define dso_local void @use_inline_header_func_unsafe_indexable_ptr(
// CHECK-SAME: i32 noundef [[COUNT:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[COUNT_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[PTR:%.*]] = alloca %"__bounds_safety::wide_ptr.bidi_indexable", align 8
// CHECK-NEXT:    [[BYVAL_TEMP:%.*]] = alloca %"__bounds_safety::wide_ptr.bidi_indexable", align 8
// CHECK-NEXT:    store i32 [[COUNT]], ptr [[COUNT_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[COUNT_ADDR]], align 4
// CHECK-NEXT:    [[CALL:%.*]] = call ptr @inline_header_func_unsafe_indexable_ptr(i32 noundef [[TMP0]])
// CHECK-NEXT:    [[IDX_EXT:%.*]] = sext i32 [[TMP0]] to i64
// CHECK-NEXT:    [[ADD_PTR:%.*]] = getelementptr inbounds i32, ptr [[CALL]], i64 [[IDX_EXT]]
// CHECK-NEXT:    [[TMP1:%.*]] = getelementptr inbounds %"__bounds_safety::wide_ptr.bidi_indexable", ptr [[PTR]], i32 0, i32 0
// CHECK-NEXT:    store ptr [[CALL]], ptr [[TMP1]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds %"__bounds_safety::wide_ptr.bidi_indexable", ptr [[PTR]], i32 0, i32 1
// CHECK-NEXT:    store ptr [[ADD_PTR]], ptr [[TMP2]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds %"__bounds_safety::wide_ptr.bidi_indexable", ptr [[PTR]], i32 0, i32 2
// CHECK-NEXT:    store ptr [[CALL]], ptr [[TMP3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 8 [[BYVAL_TEMP]], ptr align 8 [[PTR]], i64 24, i1 false)
// CHECK-NEXT:    call void @consume(ptr noundef [[BYVAL_TEMP]])
// CHECK-NEXT:    ret void
//
// LEGACY-LABEL: define dso_local void @use_inline_header_func_unsafe_indexable_ptr(
// LEGACY-SAME: i32 noundef [[COUNT:%.*]]) #[[ATTR0]] {
// LEGACY-NEXT:  [[ENTRY:.*:]]
// LEGACY-NEXT:    [[COUNT_ADDR:%.*]] = alloca i32, align 4
// LEGACY-NEXT:    [[PTR:%.*]] = alloca %"__bounds_safety::wide_ptr.bidi_indexable", align 8
// LEGACY-NEXT:    [[BYVAL_TEMP:%.*]] = alloca %"__bounds_safety::wide_ptr.bidi_indexable", align 8
// LEGACY-NEXT:    store i32 [[COUNT]], ptr [[COUNT_ADDR]], align 4
// LEGACY-NEXT:    [[TMP0:%.*]] = load i32, ptr [[COUNT_ADDR]], align 4
// LEGACY-NEXT:    [[CALL:%.*]] = call ptr @inline_header_func_unsafe_indexable_ptr(i32 noundef [[TMP0]])
// LEGACY-NEXT:    [[IDX_EXT:%.*]] = sext i32 [[TMP0]] to i64
// LEGACY-NEXT:    [[ADD_PTR:%.*]] = getelementptr inbounds i32, ptr [[CALL]], i64 [[IDX_EXT]]
// LEGACY-NEXT:    [[TMP1:%.*]] = getelementptr inbounds %"__bounds_safety::wide_ptr.bidi_indexable", ptr [[PTR]], i32 0, i32 0
// LEGACY-NEXT:    store ptr [[CALL]], ptr [[TMP1]], align 8
// LEGACY-NEXT:    [[TMP2:%.*]] = getelementptr inbounds %"__bounds_safety::wide_ptr.bidi_indexable", ptr [[PTR]], i32 0, i32 1
// LEGACY-NEXT:    store ptr [[ADD_PTR]], ptr [[TMP2]], align 8
// LEGACY-NEXT:    [[TMP3:%.*]] = getelementptr inbounds %"__bounds_safety::wide_ptr.bidi_indexable", ptr [[PTR]], i32 0, i32 2
// LEGACY-NEXT:    store ptr [[CALL]], ptr [[TMP3]], align 8
// LEGACY-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr align 8 [[BYVAL_TEMP]], ptr align 8 [[PTR]], i64 24, i1 false)
// LEGACY-NEXT:    call void @consume(ptr noundef [[BYVAL_TEMP]])
// LEGACY-NEXT:    ret void
//
void use_inline_header_func_unsafe_indexable_ptr(int count) {
  int* ptr = inline_header_func_unsafe_indexable_ptr(count);
  consume(ptr);
}
